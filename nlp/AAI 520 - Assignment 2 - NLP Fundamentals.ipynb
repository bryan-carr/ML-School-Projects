{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeqzv/3F2xiOxsXQmhC6g3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# AAI 520 - Natural Language Processing\n","### Assignment 2 - NLP Fundamentals\n","\n","by Bryan Carr\n","\n","19 September, 2022\n","\n","for University of San Diego\n","\n","Prof. Siamak Aram"],"metadata":{"id":"C33GN3gizjPF"}},{"cell_type":"markdown","source":["In this assignment, we will explore the short story \"An Occurrence at Owl Bridge Creek\" by Ambrose Brice (1890). I've copied in the PDF's instructions, and filled the notebook in as appropriate."],"metadata":{"id":"ThcD_BLzz4id"}},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"iIYuBoSQzjoc","executionInfo":{"status":"ok","timestamp":1663636224653,"user_tz":420,"elapsed":8061,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**1. Create a Doc object from the file owlcreek.txt**\n","\n","HINT: Use with open(\"../TextFiles/owlcreek.txt\") as f:"],"metadata":{"id":"U1MTiesI0swX"}},{"cell_type":"code","source":["# Enter your code here:\n","\n","import base64\n","import requests\n","\n","text_url = 'https://raw.githubusercontent.com/AkiraKane/Natural_Language_Processing_Course-/master/TextFiles/owlcreek.txt'\n","req = requests.get(text_url)\n","req = req.text\n","#print(req)  ##un-comment to view print out of text\n","\n","doc = nlp(req)\n"],"metadata":{"id":"FHeJjybm06MG","executionInfo":{"status":"ok","timestamp":1663636225563,"user_tz":420,"elapsed":919,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Run this cell to verify it worked:\n","\n","doc[:36]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ee9GD6Ec09ed","executionInfo":{"status":"ok","timestamp":1663636225564,"user_tz":420,"elapsed":9,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"046b9635-b44e-4d46-b6ed-bacf37c18456"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AN OCCURRENCE AT OWL CREEK BRIDGE\n","\n","by Ambrose Bierce\n","\n","I\n","\n","A man stood upon a railroad bridge in northern Alabama, looking down\n","into the swift water twenty feet below.  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["**2. How many tokens are contained in the file?**\n","\n","Expected Ans: 4833"],"metadata":{"id":"TB3U0qGC1Mal"}},{"cell_type":"code","source":["## Code here for Q2\n","\n","len(list(doc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beYVsBlj1SOX","executionInfo":{"status":"ok","timestamp":1663636225564,"user_tz":420,"elapsed":7,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"124d7ed2-377a-48b9-ba0b-cdcbc2266e3a"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4835"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["**3. How many sentences are contained in the file?**\n","\n","HINT: You'll want to build a list first!\n","\n","Expected ANS: 211"],"metadata":{"id":"-r3p25ZA1VCy"}},{"cell_type":"code","source":["## Code here for Q3\n","\n","list_of_sentences = list(doc.sents)\n","\n","len(list_of_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92CGy9cy1cww","executionInfo":{"status":"ok","timestamp":1663636225564,"user_tz":420,"elapsed":6,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"5be4cfbc-24ea-4c82-afe4-8c9a881fe1ac"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["204"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["**4. Print the second sentence in the document**\n","\n","HINT: Indexing starts at zero, and the title counts as the first sentence.\n","\n","ANS: A man stood upon a railroad bridge...."],"metadata":{"id":"KvuCVUd11fWd"}},{"cell_type":"code","source":["## Code here for Q4\n","\n","# spaCy is grabbing the Title, Author and Chapter number as part of sentence[0]\n","# Garinn from class has come up with this solution, slicing those early tokens - credit to him\n","\n","second_sentence = list_of_sentences[0][13:]\n","\n","print(second_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NlcMsE61rct","executionInfo":{"status":"ok","timestamp":1663636225565,"user_tz":420,"elapsed":6,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"6d4c7cc3-cc8f-4192-eebd-9983ebb88e85"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["A man stood upon a railroad bridge in northern Alabama, looking down\r\n","into the swift water twenty feet below.  \n"]}]},{"cell_type":"markdown","source":["5. For each token in the sentence above, print its << text >>, << POS >> tag, << dep >> tag, and << lemma >>.\n","\n","CHALLENGE: Have values line up in columns in the print output\n"],"metadata":{"id":"GSzHCS3o1vg6"}},{"cell_type":"code","source":["# Normal Solution\n","\n","for token in second_sentence:\n","  print (token.text, token.pos_, token.dep_, token.lemma_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iER2O9EQ19S2","executionInfo":{"status":"ok","timestamp":1663636225565,"user_tz":420,"elapsed":5,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"d49bbb2f-76d1-4b19-dd41-6dd64ab317f6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["A PROPN det A\n","man NOUN nsubj man\n","stood VERB ROOT stand\n","upon SCONJ prep upon\n","a DET det a\n","railroad NOUN compound railroad\n","bridge NOUN pobj bridge\n","in ADP prep in\n","northern ADJ amod northern\n","Alabama PROPN pobj Alabama\n",", PUNCT punct ,\n","looking VERB advcl look\n","down ADP advmod down\n","\r\n"," SPACE dep \r\n","\n","into ADP prep into\n","the DET det the\n","swift ADJ amod swift\n","water NOUN pobj water\n","twenty NUM nummod twenty\n","feet NOUN npadvmod foot\n","below ADV advmod below\n",". PUNCT punct .\n","  SPACE dep  \n"]}]},{"cell_type":"code","source":["# Challenge Solution\n","# Build a DataFrame out of the required entries\n","\n","import pandas as pd\n","\n","challenge_df = pd.DataFrame(columns=['text', 'pos', 'dep', 'lemma'])\n","\n","for token in second_sentence:\n","  challenge_df.loc[len(challenge_df)] = [token.text, token.pos_, token.dep_, token.lemma_]\n","\n","# Remove the 'dep' entries after 'SPACE', as per the example\n","challenge_df.loc[challenge_df['pos'] == 'SPACE', 'dep'] = \"\"\n","\n","# Print DataFrame while hiding Index and Column names\n","challenge_df.style.hide_index().hide_columns()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"N4_MzJ_aAei6","executionInfo":{"status":"ok","timestamp":1663636225787,"user_tz":420,"elapsed":226,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"8948d5d6-a615-4320-f800-8a411954d811"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7ff046be8bd0>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_89fda_\" class=\"dataframe\">\n","  <thead>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_89fda_row0_col0\" class=\"data row0 col0\" >A</td>\n","      <td id=\"T_89fda_row0_col1\" class=\"data row0 col1\" >PROPN</td>\n","      <td id=\"T_89fda_row0_col2\" class=\"data row0 col2\" >det</td>\n","      <td id=\"T_89fda_row0_col3\" class=\"data row0 col3\" >A</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row1_col0\" class=\"data row1 col0\" >man</td>\n","      <td id=\"T_89fda_row1_col1\" class=\"data row1 col1\" >NOUN</td>\n","      <td id=\"T_89fda_row1_col2\" class=\"data row1 col2\" >nsubj</td>\n","      <td id=\"T_89fda_row1_col3\" class=\"data row1 col3\" >man</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row2_col0\" class=\"data row2 col0\" >stood</td>\n","      <td id=\"T_89fda_row2_col1\" class=\"data row2 col1\" >VERB</td>\n","      <td id=\"T_89fda_row2_col2\" class=\"data row2 col2\" >ROOT</td>\n","      <td id=\"T_89fda_row2_col3\" class=\"data row2 col3\" >stand</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row3_col0\" class=\"data row3 col0\" >upon</td>\n","      <td id=\"T_89fda_row3_col1\" class=\"data row3 col1\" >SCONJ</td>\n","      <td id=\"T_89fda_row3_col2\" class=\"data row3 col2\" >prep</td>\n","      <td id=\"T_89fda_row3_col3\" class=\"data row3 col3\" >upon</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row4_col0\" class=\"data row4 col0\" >a</td>\n","      <td id=\"T_89fda_row4_col1\" class=\"data row4 col1\" >DET</td>\n","      <td id=\"T_89fda_row4_col2\" class=\"data row4 col2\" >det</td>\n","      <td id=\"T_89fda_row4_col3\" class=\"data row4 col3\" >a</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row5_col0\" class=\"data row5 col0\" >railroad</td>\n","      <td id=\"T_89fda_row5_col1\" class=\"data row5 col1\" >NOUN</td>\n","      <td id=\"T_89fda_row5_col2\" class=\"data row5 col2\" >compound</td>\n","      <td id=\"T_89fda_row5_col3\" class=\"data row5 col3\" >railroad</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row6_col0\" class=\"data row6 col0\" >bridge</td>\n","      <td id=\"T_89fda_row6_col1\" class=\"data row6 col1\" >NOUN</td>\n","      <td id=\"T_89fda_row6_col2\" class=\"data row6 col2\" >pobj</td>\n","      <td id=\"T_89fda_row6_col3\" class=\"data row6 col3\" >bridge</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row7_col0\" class=\"data row7 col0\" >in</td>\n","      <td id=\"T_89fda_row7_col1\" class=\"data row7 col1\" >ADP</td>\n","      <td id=\"T_89fda_row7_col2\" class=\"data row7 col2\" >prep</td>\n","      <td id=\"T_89fda_row7_col3\" class=\"data row7 col3\" >in</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row8_col0\" class=\"data row8 col0\" >northern</td>\n","      <td id=\"T_89fda_row8_col1\" class=\"data row8 col1\" >ADJ</td>\n","      <td id=\"T_89fda_row8_col2\" class=\"data row8 col2\" >amod</td>\n","      <td id=\"T_89fda_row8_col3\" class=\"data row8 col3\" >northern</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row9_col0\" class=\"data row9 col0\" >Alabama</td>\n","      <td id=\"T_89fda_row9_col1\" class=\"data row9 col1\" >PROPN</td>\n","      <td id=\"T_89fda_row9_col2\" class=\"data row9 col2\" >pobj</td>\n","      <td id=\"T_89fda_row9_col3\" class=\"data row9 col3\" >Alabama</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row10_col0\" class=\"data row10 col0\" >,</td>\n","      <td id=\"T_89fda_row10_col1\" class=\"data row10 col1\" >PUNCT</td>\n","      <td id=\"T_89fda_row10_col2\" class=\"data row10 col2\" >punct</td>\n","      <td id=\"T_89fda_row10_col3\" class=\"data row10 col3\" >,</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row11_col0\" class=\"data row11 col0\" >looking</td>\n","      <td id=\"T_89fda_row11_col1\" class=\"data row11 col1\" >VERB</td>\n","      <td id=\"T_89fda_row11_col2\" class=\"data row11 col2\" >advcl</td>\n","      <td id=\"T_89fda_row11_col3\" class=\"data row11 col3\" >look</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row12_col0\" class=\"data row12 col0\" >down</td>\n","      <td id=\"T_89fda_row12_col1\" class=\"data row12 col1\" >ADP</td>\n","      <td id=\"T_89fda_row12_col2\" class=\"data row12 col2\" >advmod</td>\n","      <td id=\"T_89fda_row12_col3\" class=\"data row12 col3\" >down</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row13_col0\" class=\"data row13 col0\" >\r\n","</td>\n","      <td id=\"T_89fda_row13_col1\" class=\"data row13 col1\" >SPACE</td>\n","      <td id=\"T_89fda_row13_col2\" class=\"data row13 col2\" ></td>\n","      <td id=\"T_89fda_row13_col3\" class=\"data row13 col3\" >\r\n","</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row14_col0\" class=\"data row14 col0\" >into</td>\n","      <td id=\"T_89fda_row14_col1\" class=\"data row14 col1\" >ADP</td>\n","      <td id=\"T_89fda_row14_col2\" class=\"data row14 col2\" >prep</td>\n","      <td id=\"T_89fda_row14_col3\" class=\"data row14 col3\" >into</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row15_col0\" class=\"data row15 col0\" >the</td>\n","      <td id=\"T_89fda_row15_col1\" class=\"data row15 col1\" >DET</td>\n","      <td id=\"T_89fda_row15_col2\" class=\"data row15 col2\" >det</td>\n","      <td id=\"T_89fda_row15_col3\" class=\"data row15 col3\" >the</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row16_col0\" class=\"data row16 col0\" >swift</td>\n","      <td id=\"T_89fda_row16_col1\" class=\"data row16 col1\" >ADJ</td>\n","      <td id=\"T_89fda_row16_col2\" class=\"data row16 col2\" >amod</td>\n","      <td id=\"T_89fda_row16_col3\" class=\"data row16 col3\" >swift</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row17_col0\" class=\"data row17 col0\" >water</td>\n","      <td id=\"T_89fda_row17_col1\" class=\"data row17 col1\" >NOUN</td>\n","      <td id=\"T_89fda_row17_col2\" class=\"data row17 col2\" >pobj</td>\n","      <td id=\"T_89fda_row17_col3\" class=\"data row17 col3\" >water</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row18_col0\" class=\"data row18 col0\" >twenty</td>\n","      <td id=\"T_89fda_row18_col1\" class=\"data row18 col1\" >NUM</td>\n","      <td id=\"T_89fda_row18_col2\" class=\"data row18 col2\" >nummod</td>\n","      <td id=\"T_89fda_row18_col3\" class=\"data row18 col3\" >twenty</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row19_col0\" class=\"data row19 col0\" >feet</td>\n","      <td id=\"T_89fda_row19_col1\" class=\"data row19 col1\" >NOUN</td>\n","      <td id=\"T_89fda_row19_col2\" class=\"data row19 col2\" >npadvmod</td>\n","      <td id=\"T_89fda_row19_col3\" class=\"data row19 col3\" >foot</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row20_col0\" class=\"data row20 col0\" >below</td>\n","      <td id=\"T_89fda_row20_col1\" class=\"data row20 col1\" >ADV</td>\n","      <td id=\"T_89fda_row20_col2\" class=\"data row20 col2\" >advmod</td>\n","      <td id=\"T_89fda_row20_col3\" class=\"data row20 col3\" >below</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row21_col0\" class=\"data row21 col0\" >.</td>\n","      <td id=\"T_89fda_row21_col1\" class=\"data row21 col1\" >PUNCT</td>\n","      <td id=\"T_89fda_row21_col2\" class=\"data row21 col2\" >punct</td>\n","      <td id=\"T_89fda_row21_col3\" class=\"data row21 col3\" >.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_89fda_row22_col0\" class=\"data row22 col0\" > </td>\n","      <td id=\"T_89fda_row22_col1\" class=\"data row22 col1\" >SPACE</td>\n","      <td id=\"T_89fda_row22_col2\" class=\"data row22 col2\" ></td>\n","      <td id=\"T_89fda_row22_col3\" class=\"data row22 col3\" > </td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["**6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text.**\n","\n","HINT: You should include an << 'IS_SPACE': True >> pattern between the two words!"],"metadata":{"id":"n1VgvBS92d_c"}},{"cell_type":"code","source":["# Import the Matcher library\n","\n","from spacy.matcher import Matcher\n","matcher = Matcher(nlp.vocab)"],"metadata":{"id":"gmlODBL32wUn","executionInfo":{"status":"ok","timestamp":1663636225787,"user_tz":420,"elapsed":25,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["## Create a pattern and add it to matcher:\n","\n","pattern = [\n","    {'TEXT': 'swimming'}, \n","    {'IS_SPACE' : True}, \n","    {'TEXT' : 'vigorously'}\n","]\n","\n","matcher.add('swim', [pattern])"],"metadata":{"id":"Vb5oRz5k2tjR","executionInfo":{"status":"ok","timestamp":1663636225788,"user_tz":420,"elapsed":24,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["## Create a list of matches called \"found_matches\" and print the list\n","## Ans: [(1288...66681, 1274, 1277), (12881...66681, 3607, 3610)]\n","\n","found_matches = matcher(doc)\n","\n","found_matches"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Vk7oN0l27Yy","executionInfo":{"status":"ok","timestamp":1663636225788,"user_tz":420,"elapsed":23,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"708578d7-44eb-46f6-88fd-83b3344768d7"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(13054409096476681252, 1274, 1277), (13054409096476681252, 3609, 3612)]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["**7. Print the text surrounding each found match**"],"metadata":{"id":"mE8Apene21zy"}},{"cell_type":"code","source":["words1 = doc[found_matches[0][1] - 9 : found_matches[0][2] + 13]\n","words1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NeA671F3Xad","executionInfo":{"status":"ok","timestamp":1663636225789,"user_tz":420,"elapsed":18,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"b7255aaf-6245-4135-d970-edec27aa8df3"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["By diving I could evade the bullets and, swimming\n","vigorously, reach the bank, take to the woods and get away home"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["words2 = doc[found_matches[1][1] - 7 : found_matches[1][2] + 5]\n","words2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K80PXj1F3Xii","executionInfo":{"status":"ok","timestamp":1663636225789,"user_tz":420,"elapsed":14,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"5ca14bdc-cf01-4a3c-d3ef-f093c8a4ec9d"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["over his shoulder; he was now swimming\n","vigorously with the current.  "]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["**EXTRA CREDIT: Print the Sentence that contains each found match**"],"metadata":{"id":"MskK0Atn3YLf"}},{"cell_type":"code","source":["# We can simply call the Sentence object that contains our words.\n","# For the first example, it only needs to capture the additional Period token:\n","\n","words1.sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNy6-b7dVbua","executionInfo":{"status":"ok","timestamp":1663636225789,"user_tz":420,"elapsed":12,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"7488539d-e727-4b58-e8b7-302847d07f6e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["By diving I could evade the bullets and, swimming\n","vigorously, reach the bank, take to the woods and get away home.  "]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# We could also call it referencing the words' index\n","\n","doc[found_matches[1][1] : found_matches[1][2]].sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh4e-MvHYkpf","executionInfo":{"status":"ok","timestamp":1663636225953,"user_tz":420,"elapsed":173,"user":{"displayName":"Bryan Carr","userId":"11690414491872737439"}},"outputId":"3c43d6cc-6953-4db2-891c-36865ea10ed3"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["The hunted man saw all this over his shoulder; he was now swimming\n","vigorously with the current.  "]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### **References**\n","\n","I referred to Stack Overflow for help importing, particularily these two results:\n","\n","https://stackoverflow.com/questions/61723047/how-to-read-txt-files-from-github-into-google-colab\n","\n","https://stackoverflow.com/questions/38491722/reading-a-github-file-using-python-returns-html-tags/38497199#38497199\n","\n"],"metadata":{"id":"w2DGEZ_x5YhI"}}]}